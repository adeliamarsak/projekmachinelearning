# -*- coding: utf-8 -*-
"""[Clustering] Submission Akhir BMLP_Adelia Marsa Karunika.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dG9t9H318BbIF62YHVogyGuYqQyGChXK

# **1. Perkenalan Dataset**

Tahap pertama, Anda harus mencari dan menggunakan dataset **tanpa label** dengan ketentuan sebagai berikut:

1. **Sumber Dataset**:  
   Dataset dapat diperoleh dari berbagai sumber, seperti public repositories (*Kaggle*, *UCI ML Repository*, *Open Data*) atau data primer yang Anda kumpulkan sendiri.
   
2. **Ketentuan Dataset**:
   - **Tanpa label**: Dataset tidak boleh memiliki label atau kelas.
   - **Jumlah Baris**: Minimal 1000 baris untuk memastikan dataset cukup besar untuk analisis yang bermakna.
   - **Tipe Data**: Harus mengandung data **kategorikal** dan **numerikal**.
     - *Kategorikal*: Misalnya jenis kelamin, kategori produk.
     - *Numerikal*: Misalnya usia, pendapatan, harga.

3. **Pembatasan**:  
   Dataset yang sudah digunakan dalam latihan clustering (seperti customer segmentation) tidak boleh digunakan.

Dataset diambil dari https://raw.githubusercontent.com/dodyks/Clustering-Model/refs/heads/main/clustering_data.csv

# **2. Import Library**

Pada tahap ini, Anda perlu mengimpor beberapa pustaka (library) Python yang dibutuhkan untuk analisis data dan pembangunan model machine learning.
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import time

import sklearn
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import silhouette_score, accuracy_score, f1_score, confusion_matrix, precision_score, recall_score, f1_score

from sklearn.decomposition import PCA
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier


from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.naive_bayes import GaussianNB

"""# **3. Memuat Dataset**

Pada tahap ini, Anda perlu memuat dataset ke dalam notebook. Jika dataset dalam format CSV, Anda bisa menggunakan pustaka pandas untuk membacanya. Pastikan untuk mengecek beberapa baris awal dataset untuk memahami strukturnya dan memastikan data telah dimuat dengan benar.

Jika dataset berada di Google Drive, pastikan Anda menghubungkan Google Drive ke Colab terlebih dahulu. Setelah dataset berhasil dimuat, langkah berikutnya adalah memeriksa kesesuaian data dan siap untuk dianalisis lebih lanjut.
"""

# Data Loading
df = pd.read_csv('https://raw.githubusercontent.com/dodyks/Clustering-Model/refs/heads/main/clustering_data.csv')
df.head()

"""# **4. Exploratory Data Analysis (EDA)**

Pada tahap ini, Anda akan melakukan **Exploratory Data Analysis (EDA)** untuk memahami karakteristik dataset. EDA bertujuan untuk:

1. **Memahami Struktur Data**
   - Tinjau jumlah baris dan kolom dalam dataset.  
   - Tinjau jenis data di setiap kolom (numerikal atau kategorikal).

2. **Menangani Data yang Hilang**  
   - Identifikasi dan analisis data yang hilang (*missing values*). Tentukan langkah-langkah yang diperlukan untuk menangani data yang hilang, seperti pengisian atau penghapusan data tersebut.

3. **Analisis Distribusi dan Korelasi**  
   - Analisis distribusi variabel numerik dengan statistik deskriptif dan visualisasi seperti histogram atau boxplot.  
   - Periksa hubungan antara variabel menggunakan matriks korelasi atau scatter plot.

4. **Visualisasi Data**  
   - Buat visualisasi dasar seperti grafik distribusi dan diagram batang untuk variabel kategorikal.  
   - Gunakan heatmap atau pairplot untuk menganalisis korelasi antar variabel.

Tujuan dari EDA adalah untuk memperoleh wawasan awal yang mendalam mengenai data dan menentukan langkah selanjutnya dalam analisis atau pemodelan.

## 1. **Memahami Struktur Data**

Tinjau jumlah baris dan kolom dalam dataset.
Tinjau jenis data di setiap kolom (numerikal atau kategorikal).
"""

df.info()

"""Terdapat perbedaan jumlah pada kolom Usia"""

#statistik deskriptif dari dataset yang digunakan
df.describe(include="all")

# prompt: kolom numerik

# Identify numerical columns
kolomnumerik = df.select_dtypes(include=['number']).columns
# Print the numerical columns
kolomnumerik

# prompt: kolom kategorik

# Identify categorical columns
kolomkategorik = df.select_dtypes(include=['object']).columns
# Print the categorical columns
kolomkategorik

"""## 2. Data Understanding"""

df.groupby(('Area'))['Area'].count()

df.groupby(('Vintage'))['Vintage'].count()

df.groupby(('Jalur_Pembukaan'))['Jalur_Pembukaan'].count()

df.groupby(('Status_Perkawinan'))['Status_Perkawinan'].count()

"""**5. Analisis Distribusi dan Korelasi**

Analisis distribusi variabel numerik dengan statistik deskriptif dan visualisasi seperti histogram atau boxplot.
Periksa hubungan antara variabel menggunakan matriks korelasi atau scatter plot.

**6. Visualisasi Data**

Buat visualisasi dasar seperti grafik distribusi dan diagram batang untuk variabel kategorikal.
Gunakan heatmap atau pairplot untuk menganalisis korelasi antar variabel.

# **5. Data Preprocessing**

Pada tahap ini, data preprocessing adalah langkah penting untuk memastikan kualitas data sebelum digunakan dalam model machine learning. Data mentah sering kali mengandung nilai kosong, duplikasi, atau rentang nilai yang tidak konsisten, yang dapat memengaruhi kinerja model. Oleh karena itu, proses ini bertujuan untuk membersihkan dan mempersiapkan data agar analisis berjalan optimal.

Berikut adalah tahapan-tahapan yang bisa dilakukan, tetapi **tidak terbatas** pada:
1. Menghapus atau Menangani Data Kosong (Missing Values)
2. Menghapus Data Duplikat
3. Normalisasi atau Standarisasi Fitur
4. Deteksi dan Penanganan Outlier
5. Encoding Data Kategorikal
6. Binning (Pengelompokan Data)

Cukup sesuaikan dengan karakteristik data yang kamu gunakan yah.

## 1. Menghapus atau Menangani Data Kosong (Missing Values) dan Data Duplikat
"""

df.isnull().sum()

df = df.dropna()
df.isnull().sum()

df.duplicated().sum()

"""**Penanganan Outlier**"""

for feature in kolomnumerik:
    plt.figure(figsize=(5, 3))
    sns.boxplot(x=df[feature])
    plt.title(f'Box Plot of {feature}')
    plt.show()

Q1 = df[kolomnumerik].quantile(0.25)
Q3 = df[kolomnumerik].quantile(0.75)
IQR = Q3 - Q1

# Filter dataframe untuk hanya menyimpan baris yang tidak mengandung outliers pada kolom numerik dan Menggabungkan kembali dengan kolom kategorikal

# Identify numerical and categorical columns
kolomnumerik = df.select_dtypes(include=['number']).columns
kolomkategorikal = df.select_dtypes(exclude=['number']).columns

# Calculate IQR and identify outliers for numerical features
Q1 = df[kolomnumerik].quantile(0.25)
Q3 = df[kolomnumerik].quantile(0.75)
IQR = Q3 - Q1
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

# Filter out outliers
df_filtered = df[~((df[kolomnumerik] < lower_bound) | (df[kolomnumerik] > upper_bound)).any(axis=1)]

# Concatenate with categorical columns
df_final = pd.concat([df_filtered[kolomnumerik], df[kolomkategorikal]], axis=1)

print(df_final.head())

"""## 2. Pengecekan Outlier"""

df.count()

#mendeteksi data outlier (menggunakan Z-Score)
from scipy import stats
z_scores = stats.zscore(df[['Usia', 'Pendapatan_Tahunan', 'Total_Relationship_Balance']])
df = df[(z_scores < 3).all(axis=1)] # menghaps data dengan Z-Score lebih dari 3

df.count()

"""## Filtering Data Telemakerting Only"""

df_0 = df[df['Jalur_Pembukaan']== 'Telemarketing']

df_0

df_0 = df_0.drop(columns=['GCIF', 'Jalur_Pembukaan']).reset_index(drop=True)

df_0

"""## Pembagian Data Eksperimen

Experiment 0 --> Semua Variabel Digunakan
Experiment 1 --> Menggunakan Demographics
Experiment 2 --> Menggunakan Financial Related Variable
"""

df_1 = df_0.iloc[:,0:7]

df_1

df_2 = df_0.iloc[:,7:17]

df_2

"""## Melakukan Encoding untuk Data Category (Dummy)"""

df_1 = pd.get_dummies(df_1, columns=['Area','Jenis_Kelamin', 'Status_Perkawinan', 'Pendidikan', 'Vintage'])

df_1

"""## Standarisasi Data Numerik"""

predname_num = df_2.columns
predname_num

from sklearn.preprocessing import StandardScaler
pt = StandardScaler()
X_num = pd.DataFrame(pt.fit_transform(df_2)) # Assign the scaled data to X_num

X_num.head()

X_num.columns = predname_num
X_num.head()

"""## Pengecekan Korelasi"""

corrtest1 = df_1.corr().abs()
corrtest2 = X_num.corr().abs()

df_1

X_num

#gabungan df_1 dan X_num
data_gabung = pd.concat([df_1, X_num], axis=1, join='inner')
data_gabung

"""# **6. Pembangunan Model Clustering**

## **a. Pembangunan Model Clustering**

Pada tahap ini, Anda membangun model clustering dengan memilih algoritma yang sesuai untuk mengelompokkan data berdasarkan kesamaan. Berikut adalah **rekomendasi** tahapannya.
1. Pilih algoritma clustering yang sesuai.

  Model akan menggunakan 2 jenis algoritma clustering:

  **K-Means**
  
  **K=Medoid**

2. Latih model dengan data menggunakan algoritma tersebut.
"""

# prompt: Menentukan Jumlah Cluster Optimal (Elbow Method)

# Elbow Method
inertia = []
for i in range(1, 11):
    kmeans = KMeans(n_clusters=i, random_state=42)
    kmeans.fit(df[kolomnumerik])
    inertia.append(kmeans.inertia_)

plt.plot(range(1, 11), inertia, marker='o')
plt.title('Elbow Method for Optimal k')
plt.xlabel('Number of clusters')
plt.ylabel('Inertia')
plt.show()

"""### **K-Means**"""

# prompt: menentukan jumlah cluster optimal (ganti sesuai hasil Elbow Method), terapkan K-Means, Tambahkan hasil clustering ke dataset asli, Visualisasi Distribusi Cluster (Jika ada 2 fitur numerik dominan)

# Determine the optimal number of clusters (replace with the result from the Elbow Method)
optimal_k = 4  # Example: Replace with the optimal k value from the Elbow Method

# Apply K-Means clustering
kmeans = KMeans(n_clusters=optimal_k, random_state=42)
df['cluster'] = kmeans.fit_predict(df[kolomnumerik])

# Visualize cluster distribution (if there are 2 dominant numerical features)
if len(kolomnumerik) >= 2:
    plt.figure(figsize=(10, 8))
    sns.scatterplot(x=df[kolomnumerik[0]], y=df[kolomnumerik[1]], hue=df['cluster'], palette='viridis')
    plt.title('Cluster Distribution')
    plt.xlabel(kolomnumerik[0])
    plt.ylabel(kolomnumerik[1])
    plt.show()

# varisi hyperparameter (jumlah cluster)
# experiment 0
for n_clusters in range(3, 6):
    kmeans = KMeans(n_clusters=n_clusters, random_state=0)
    cluster_labels = kmeans.fit_predict(data_gabung)

# varisi hyperparameter (jumlah cluster)
# experiment 1
for n_clusters in range(3, 6):
  kmeans = KMeans(n_clusters=n_clusters, random_state=0)
  cluster_labels = kmeans.fit_predict(df_1)

# varisi hyperparameter (jumlah cluster)
# eksperimen 2
for n_clusters in range(3, 6):
  kmeans = KMeans(n_clusters=n_clusters, random_state=0)
  cluster_labels = kmeans.fit_predict(X_num)

"""### **K-Medoids**"""

!pip install scikit-learn-extra

from sklearn_extra.cluster import KMedoids

#eksperimen 0
for n_clusters in range(3, 6):
    kmedoids = KMedoids(n_clusters=n_clusters, random_state=0)
    cluster_labels = kmedoids.fit_predict(data_gabung)

#eksperimen 1
for n_clusters in range(3, 6):
    kmedoids = KMedoids(n_clusters=n_clusters, random_state=0)
    cluster_labels = kmedoids.fit_predict(df_1)

#eksperimen 3
for n_clusters in range(3, 6):
    kmedoids = KMedoids(n_clusters=n_clusters, random_state=0)
    cluster_labels = kmedoids.fit_predict(X_num)

"""## **b. Evaluasi Model Clustering**

Untuk menentukan jumlah cluster yang optimal dalam model clustering, Anda dapat menggunakan metode Elbow atau Silhouette Score.

Metode ini membantu kita menemukan jumlah cluster yang memberikan pemisahan terbaik antar kelompok data, sehingga model yang dibangun dapat lebih efektif. Berikut adalah **rekomendasi** tahapannya.
1. Gunakan Silhouette Score dan Elbow Method untuk menentukan jumlah cluster optimal.
2. Hitung Silhouette Score sebagai ukuran kualitas cluster.
"""

# prompt: Menentukan Jumlah Cluster Optimal (Elbow Method)

# Elbow Method
inertia = []
for i in range(1, 11):
    kmeans = KMeans(n_clusters=i, random_state=42)
    kmeans.fit(df[kolomnumerik])
    inertia.append(kmeans.inertia_)

plt.plot(range(1, 11), inertia, marker='o')
plt.title('Elbow Method for Optimal k')
plt.xlabel('Number of clusters')
plt.ylabel('Inertia')
plt.show()

"""## Hitung Silhouette Score K-Means"""

# varisi hyperparameter (jumlah cluster)
# eksperimen 0
for n_clusters in range(3, 6):
    kmeans = KMeans(n_clusters=n_clusters, random_state=0)
    cluster_labels = kmeans.fit_predict(data_gabung)
    silhouette_avg = silhouette_score(data_gabung, cluster_labels)
    print(f"Silhouette Score (K-Means) -"+str(n_clusters)+" : "+str(silhouette_avg))
    df_0['Clustering_KMeans_Exp0_'+str(n_clusters)] = cluster_labels

"""**Silhouette Score $< 0.55$** sehingga tidak memenuhi untuk data yang menggunakan semua variabel"""

#eksperimen 1
for n_clusters in range(3, 6):
  kmeans = KMeans(n_clusters=n_clusters, random_state=0)
  cluster_labels = kmeans.fit_predict(df_1)
  silhouette_avg = silhouette_score(df_1, cluster_labels)
  print(f"Silhouette Score (K-Mens) -"+str(n_clusters)+" : "+str(silhouette_avg))
  df_0['Clustering_KMeans_Exp1_'+str(n_clusters)] =cluster_labels

"""**Silhouette Score $< 0.55$** sehingga tidak memenuhi untuk data yang menggunakan demographics"""

# varisi hyperparameter (jumlah cluster)
#eksperimen 2
for n_clusters in range(3, 6):
  kmeans = KMeans(n_clusters=n_clusters, random_state=0)
  cluster_labels = kmeans.fit_predict(X_num)
  silhouette_avg = silhouette_score(X_num, cluster_labels)
  print(f"Silhouette Score (K-Mens) -"+str(n_clusters)+" : "+str(silhouette_avg))
  df_0['Clustering_KMeans_Exp2_'+str(n_clusters)] =cluster_labels

"""**Silhouette Score $< 0.55$** sehingga tidak memenuhi untuk data yang menggunakan Financial Related Variable

## **c. Feature Selection (Opsional)**

Silakan lakukan feature selection jika Anda membutuhkan optimasi model clustering. Jika Anda menerapkan proses ini, silakan lakukan pemodelan dan evaluasi kembali menggunakan kolom-kolom hasil feature selection. Terakhir, bandingkan hasil performa model sebelum dan sesudah menerapkan feature selection.

### Feature Selection K-Means menggunakan PCA
"""

# prompt: pca dan cek silhouette score

from sklearn.decomposition import PCA

# Assuming data_combined, data1, and X_num are defined as in your original code

# Experiment 0 with PCA
for n_clusters in range(3, 6):
    pca = PCA(n_components=1)  # Reduce to 2 principal components
    data_gabung_pca = pca.fit_transform(data_gabung)
    kmeans = KMeans(n_clusters=n_clusters, random_state=0)
    cluster_labels = kmeans.fit_predict(data_gabung_pca)
    silhouette_avg = silhouette_score(data_gabung_pca, cluster_labels)
    print(f"Silhouette Score (K-Means with PCA) - Exp0 - {n_clusters} clusters: {silhouette_avg}")

# Experiment 1 with PCA
for n_clusters in range(3, 6):
    pca = PCA(n_components=1)
    df_1_pca = pca.fit_transform(df_1)
    kmeans = KMeans(n_clusters=n_clusters, random_state=0)
    cluster_labels = kmeans.fit_predict(df_1_pca)
    silhouette_avg = silhouette_score(df_1_pca, cluster_labels)
    print(f"Silhouette Score (K-Means with PCA) - Exp1 - {n_clusters} clusters: {silhouette_avg}")

# Experiment 2 with PCA
for n_clusters in range(3, 6):
    pca = PCA(n_components=1)
    X_num_pca = pca.fit_transform(X_num)
    kmeans = KMeans(n_clusters=n_clusters, random_state=0)
    cluster_labels = kmeans.fit_predict(X_num_pca)
    silhouette_avg = silhouette_score(X_num_pca, cluster_labels)
    print(f"Silhouette Score (K-Means with PCA) - Exp2 - {n_clusters} clusters: {silhouette_avg}")

"""Setelah menggunakan PCA, diperoleh Silhouette Score > 0.55 untuk

1. Data semua Variabel, dengan jumlah cluster 3, 4, dan 5

2. Data menggunakan Demographics, dengan jumlah cluster 3, 4, dan 5

3. Data menggunakan Financial Related Variable, dengan jumlah cluster 3

serta diperoleh Silhouette Score > 0.55 untuk Data menggunakan Financial Related Variable, dengan jumlah cluster 4 dan 5

## **d. Visualisasi Hasil Clustering**

Setelah model clustering dilatih dan jumlah cluster optimal ditentukan, langkah selanjutnya adalah menampilkan hasil clustering melalui visualisasi.

Berikut adalah **rekomendasi** tahapannya.
1. Tampilkan hasil clustering dalam bentuk visualisasi, seperti grafik scatter plot atau 2D PCA projection.

### Scatter Plot Hasil Clustering
"""

# Visualize cluster distribution (if there are 2 dominant numerical features)
if len(kolomnumerik) >= 2:
    plt.figure(figsize=(10, 8))
    sns.scatterplot(x=df[kolomnumerik[0]], y=df[kolomnumerik[1]], hue=df['cluster'], palette='viridis')
    plt.title('Cluster Distribution')
    plt.xlabel(kolomnumerik[0])
    plt.ylabel(kolomnumerik[1])
    plt.show()

# If you have more than 2 numerical features, use PCA for visualization
from sklearn.decomposition import PCA

if len(kolomnumerik) > 2:
    pca = PCA(n_components=2)  # Reduce to 2 principal components
    principal_components = pca.fit_transform(df[kolomnumerik])

    plt.figure(figsize=(10, 8))
    sns.scatterplot(x=principal_components[:, 0], y=principal_components[:, 1], hue=df['cluster'], palette='viridis')
    plt.title('Cluster Distribution (PCA)')
    plt.xlabel('Principal Component 1')
    plt.ylabel('Principal Component 2')
    plt.show()

"""### 2D PCA projection"""

# prompt: Tampilkan hasil clustering dalam bentuk visualisasi 2D PCA projection.

import matplotlib.pyplot as plt
from sklearn.decomposition import PCA

kmeans = KMeans(n_clusters=4, random_state=0)
cluster_labels = kmeans.fit_predict(X_num)


pca = PCA(n_components=2)
principal_components = pca.fit_transform(X_num)

plt.figure(figsize=(10, 8))
plt.scatter(principal_components[:, 0], principal_components[:, 1], c=cluster_labels, cmap='viridis')
plt.title('2D PCA Projection of Clusters')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.colorbar(label='Cluster')
plt.show()

"""## **e. Analisis dan Interpretasi Hasil Cluster**

Setelah melakukan clustering, langkah selanjutnya adalah menganalisis karakteristik dari masing-masing cluster berdasarkan fitur yang tersedia.

Berikut adalah **rekomendasi** tahapannya.
1. Analisis karakteristik tiap cluster berdasarkan fitur yang tersedia (misalnya, distribusi nilai dalam cluster).
2. Berikan interpretasi: Apakah hasil clustering sesuai dengan ekspektasi dan logika bisnis? Apakah ada pola tertentu yang bisa dimanfaatkan?
"""

# prompt: Inisialisasi dan melatih data X_num dengan model KMeans with PCA dengan jumlah cluster = 5 dan Fungsi untuk analisis karakteristik cluster, cluster_characteristics = analyze_cluster_characteristics(df, kolomnumerik)
# cluster_characteristics

from sklearn.decomposition import PCA
from sklearn.cluster import KMeans

# Inisialisasi PCA dengan jumlah komponen yang diinginkan
pca = PCA(n_components=1)  # Contoh: Mengurangi dimensi menjadi 1

# Melakukan transformasi PCA pada data X_num
X_num_pca = pca.fit_transform(X_num)

# Inisialisasi dan melatih model KMeans dengan data hasil PCA
kmeans = KMeans(n_clusters=5, random_state=0)
kmeans.fit(X_num_pca)

# Mendapatkan label cluster untuk setiap data point
cluster_labels = kmeans.labels_

# Fungsi untuk analisis karakteristik cluster
def analyze_cluster_characteristics(df, kolomnumerik):
    # Menggabungkan label cluster dengan dataframe asli
    df['cluster'] = cluster_labels

    # Menampilkan statistik deskriptif untuk setiap cluster
    cluster_stats = df.groupby('cluster')[kolomnumerik].describe()
    return cluster_stats

# Memanggil fungsi untuk analisis karakteristik cluster
cluster_characteristics = analyze_cluster_characteristics(df_0, X_num.columns)
cluster_characteristics

"""Tulis hasil interpretasinya di sini.
1. Cluster 0:
Produk Tabungan: Sebagian besar nasabah di cluster ini memiliki tabungan (mean = 0.87). Ini menunjukkan mayoritas nasabah menggunakan produk tabungan.

  Produk Deposito: Sebagian kecil nasabah memiliki deposito (mean = 0.69). Deposito kurang diminati oleh nasabah di cluster ini.

  Total Relationship Balance: Saldo rata-rata sebesar 426 juta dengan standar deviasi yang tinggi, menunjukkan adanya perbedaan besar antara nasabah dalam cluster ini.

  Analisis : Nasabah pada cluster ini memiliki jumlah saldo menengah dan preferensi terhadap tabungan lebih tinggi daripada deposito.


2. Cluster 1: Produk Tabungan: Mirip dengan cluster 0, sebagian besar nasabah memiliki tabungan (mean = 0.87).

  Produk Deposito: Proporsi nasabah dengan deposito sangat rendah (mean = 0.33), menunjukkan produk ini tidak menjadi pilihan utama di cluster ini.

  Total Relationship Balance: Saldo rata-rata sebesar 330 juta, lebih rendah dibandingkan cluster 0.

  Analisis : Nasabah pada cluster 1 memiliki dengan saldo lebih rendah daripada cluster 1, sedikit yang menggunakan deposito.

3. Cluster 2 : Produk Tabungan: Mayoritas nasabah memiliki tabungan (mean = 0.93), menunjukkan preferensi kuat untuk produk ini.

  Produk Deposito: Proporsi nasabah dengan deposito lebih tinggi dibandingkan cluster 1 (mean = 0.55).

  Total Relationship Balance: Saldo rata-rata sebesar 350 juta, sedikit lebih tinggi dibandingkan cluster 1.

  Analisis : Nasabah pada cluster 2 memiliki saldo rata-rata dan preferensi kuat terhadap tabungan.

4. Cluster 3: Produk Tabungan: Hampir semua nasabah di cluster ini memiliki tabungan (mean = 0.99).

  Produk Deposito: Deposito juga lebih banyak digunakan (mean = 0.73) dibandingkan cluster lain.
  
  Total Relationship Balance: Saldo rata-rata sebesar 257 juta, menunjukkan bahwa nasabah di cluster ini mungkin berada di kelas menengah.

  Analisis : Nasabah pada cluster 3 memiliki dengan saldo lebih rendah tetapi cenderung menggunakan deposito lebih sering dibandingkan cluster lainnya.
  

5. Cluster 4: Produk Tabungan: Rata-rata penggunaan tabungan lebih rendah dibandingkan cluster lain (mean = 0.82).

  Produk Deposito: Proporsi nasabah dengan deposito paling tinggi di antara semua cluster (mean = 0.88).

  Total Relationship Balance: Saldo rata-rata sebesar 404 juta, menunjukkan potensi nasabah dengan aset besar.

  Analisis : Nasabah pada cluster ini memiliki saldo besar dan proporsi penggunaan deposito paling tinggi, menunjukkan bahwa mereka adalah kelompok potensial untuk produk investasi.

# **7. Mengeksport Data**

Simpan hasilnya ke dalam file CSV.

**Data dengan silhouette score > 0.70 adalah data X_num, yaitu data yang menggunakan Financial Related Variable dengan menggunakan metode K-Means Clustering dengan Feature Selection PCA.**
"""

# prompt: Simpan hasilnya ke dalam file CSV.

X_num.to_csv('clustering_results.csv', index=False)